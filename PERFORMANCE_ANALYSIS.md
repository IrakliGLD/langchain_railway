# Performance Analysis - Slow Response Time Issue

**Date:** 2025-10-24
**Total Request Time:** 28.88 seconds
**Primary Bottleneck:** LLM summarization (26 seconds)

---

## üìä Timing Breakdown

| Phase | Duration | Status |
|-------|----------|--------|
| Request received | 0.0s | - |
| LLM Plan/SQL generation | 2.9s | ‚úÖ Acceptable |
| SQL execution | 0.11s | ‚úÖ Very fast |
| **LLM Summarization** | **25.9s** | ‚ö†Ô∏è **CRITICAL BOTTLENECK** |
| Chart generation | <0.1s | ‚úÖ Fast |
| **Total** | **28.88s** | ‚õî Too slow |

---

## üîç Root Cause Analysis

### Primary Issue: LLM Summarization Taking 26 Seconds

**Expected:** 2-5 seconds for Gemini 2.5 Flash
**Actual:** 26 seconds (5-13x slower than expected)

**Possible Causes:**

1. **Network Latency** (Most Likely)
   - Railway ‚Üí Google Cloud API round-trip time
   - Railway hobby plan may have limited network bandwidth
   - Geographic routing inefficiencies
   - No CDN or regional optimization

2. **API Throttling**
   - Gemini API rate limiting
   - Shared quota on free tier
   - Cold start penalties

3. **Large Prompt Size**
   - Even with selective domain knowledge, prompt is ~2000-3000 tokens
   - Domain knowledge guidelines add ~1000 tokens
   - Few-shot examples add ~500 tokens

4. **Railway Hobby Plan Limitations**
   - Shared CPU (throttling during I/O waits)
   - Limited memory (512MB-1GB typical)
   - No guaranteed resources
   - Shared network bandwidth

---

## üöÄ Recommended Solutions

### IMMEDIATE (Deploy Today)

#### 1. **Add LLM Request Timeout** (Priority: CRITICAL)
```python
# In get_gemini() and get_openai()
return ChatGoogleGenerativeAI(
    model=GEMINI_MODEL,
    google_api_key=GOOGLE_API_KEY,
    temperature=0,
    convert_system_message_to_human=True,
    timeout=15,  # Add 15-second timeout
    max_retries=1  # Reduce retries
)
```
**Impact:** Fail fast instead of hanging for 26s

#### 2. **Limit Response Length**
```python
# Add max_tokens to reduce response size
return ChatGoogleGenerativeAI(
    model=GEMINI_MODEL,
    google_api_key=GOOGLE_API_KEY,
    temperature=0,
    convert_system_message_to_human=True,
    timeout=15,
    max_tokens=500,  # Limit response to ~500 tokens
)
```
**Impact:** Faster generation, smaller responses

#### 3. **Reduce Prompt Size for Simple Queries**
```python
def should_use_full_prompt(user_query: str, row_count: int) -> bool:
    """Determine if query needs full domain knowledge."""
    # Simple queries with few rows don't need full context
    if row_count <= 20 and len(user_query) < 100:
        return False
    # Trend queries need full context
    if any(kw in user_query.lower() for kw in ["trend", "why", "cause", "explain"]):
        return True
    return False

# In llm_summarize():
if should_use_full_prompt(user_query, len(rows)):
    domain_json = get_relevant_domain_knowledge(user_query, use_cache=False)
    # Use full prompt with guidelines
else:
    domain_json = "{}"  # Minimal context
    # Use shorter prompt without guidelines
```
**Impact:** 50% faster for simple queries

#### 4. **Use Cached Domain Knowledge for Non-Analytical Queries**
```python
# Detect query complexity
is_analytical = detect_analysis_mode(user_query) == "analyst"

# Use cached JSON for simple queries (faster)
domain_json = get_relevant_domain_knowledge(
    user_query,
    use_cache=not is_analytical  # Cache for simple queries
)
```
**Impact:** Skip JSON serialization for 50% of queries

---

### SHORT-TERM (This Week)

#### 5. **Implement Response Caching**
```python
from functools import lru_cache
from hashlib import md5

def get_cache_key(query: str, data: str) -> str:
    """Generate cache key from query and data."""
    return md5(f"{query}:{data[:1000]}".encode()).hexdigest()

# Simple in-memory cache
response_cache = {}  # In production, use Redis

def llm_summarize_cached(user_query, data_preview, stats_hint, lang_instruction):
    cache_key = get_cache_key(user_query, data_preview)

    if cache_key in response_cache:
        log.info("‚úÖ Using cached LLM response")
        return response_cache[cache_key]

    result = llm_summarize(user_query, data_preview, stats_hint, lang_instruction)
    response_cache[cache_key] = result
    return result
```
**Impact:** Instant responses for repeated queries

#### 6. **Parallel LLM Calls (If Multiple Needed)**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def call_llm_async(func, *args):
    """Call LLM in thread pool to enable parallelism."""
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as pool:
        return await loop.run_in_executor(pool, func, *args)

# If you need both plan and summarize:
plan_task = call_llm_async(llm_generate_plan_and_sql, query, mode, lang)
summary_task = call_llm_async(llm_summarize, query, preview, stats, lang)
plan, summary = await asyncio.gather(plan_task, summary_task)
```
**Impact:** Could reduce total time if multiple LLM calls needed

---

### MEDIUM-TERM (Railway Upgrade)

#### 7. **Upgrade Railway Plan** (Most Effective)

**Current: Hobby Plan**
- Shared CPU
- 512MB RAM
- Shared network
- $5/month

**Recommended: Pro Plan**
- Dedicated CPU
- 8GB RAM
- Better network
- $20/month

**Expected Improvement:** 50-70% faster LLM responses (5-10s instead of 26s)

#### 8. **Use Streaming Responses**
```python
# Stream tokens as they arrive instead of waiting for complete response
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = ChatGoogleGenerativeAI(
    model=GEMINI_MODEL,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)
```
**Impact:** User sees partial results immediately

---

## üêõ Chart Zero Values Issue

**Separate Issue:** Chart showing zero values despite having data

**Analysis:**
```
returned 11 rows
labels=['·É¨·Éî·Éö·Éò', '·É°·Éê·É®·É£·Éê·Éö·Éù ·É°·Éê·Éë·Éê·Éö·Éê·Éú·É°·Éù ·É§·Éê·É°·Éò ']
```

**Likely Cause:** Georgian column name encoding/parsing issue

**Solution:**
```python
# In chart generation, ensure proper encoding
def clean_column_name(name: str) -> str:
    """Clean column names for chart labels."""
    # Decode Georgian unicode properly
    return name.strip().encode('utf-8').decode('utf-8')

# Check for zero values before charting
if df[value_col].sum() == 0:
    log.warning(f"‚ö†Ô∏è Column {value_col} contains all zeros")
    # Try alternate column or skip chart
```

---

## üìà Expected Performance After Optimizations

| Optimization | Current | After | Improvement |
|--------------|---------|-------|-------------|
| LLM timeout | 26s | 15s max | 42% faster |
| Max tokens | 26s | 18-20s | 23-30% faster |
| Cached responses | 26s | 0.1s | 99% faster (cache hit) |
| Smaller prompts | 26s | 15-18s | 31-42% faster |
| Railway upgrade | 26s | 5-10s | 62-81% faster |
| **Combined** | **28.88s** | **5-12s** | **58-83% faster** |

---

## üéØ Action Plan Priority

### Phase 1: Immediate (Deploy in 1 hour)
1. ‚úÖ Add LLM timeout (15s)
2. ‚úÖ Add max_tokens (500)
3. ‚úÖ Use cached domain knowledge for simple queries
4. ‚úÖ Fix chart zero values issue

**Expected Result:** 12-15 second response time (50% improvement)

### Phase 2: Short-term (Deploy this week)
5. ‚è±Ô∏è Implement response caching
6. ‚è±Ô∏è Reduce prompt size for simple queries

**Expected Result:** 5-8 second response time (70-80% improvement)

### Phase 3: Medium-term (Next month)
7. üí∞ Upgrade Railway plan to Pro
8. üîÑ Implement streaming responses

**Expected Result:** 2-5 second response time (90% improvement)

---

## üí° Additional Recommendations

### Monitoring
- Add LLM response time tracking to `/metrics` endpoint
- Set up alerts for >10s response times
- Track cache hit rates

### User Experience
- Add loading indicators with progress updates
- Show partial results as they arrive (streaming)
- Cache common queries at application level

### Cost Optimization
- Monitor Gemini API costs (currently $0.00015 per 1K tokens)
- Estimated monthly cost: $5-20 for 10K requests
- Railway upgrade ($15/month) would pay for itself in better UX

---

## üìù Conclusion

**Primary Issue:** LLM summarization taking 26 seconds due to Railway network limitations

**Quick Fix:** Add timeouts + reduce prompt size = 50% improvement today

**Best Long-term Fix:** Upgrade Railway plan = 80%+ improvement

**Cost-Benefit:** $15/month upgrade saves 20+ seconds per request
